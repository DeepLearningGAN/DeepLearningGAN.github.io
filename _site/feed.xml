<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-02-21T16:50:03+05:30</updated><id>http://localhost:4000/feed.xml</id><title type="html">Expedite AI.</title><subtitle>Illustrative blogs on state of the art deep learning models.</subtitle><entry><title type="html">A Radiance Field Approach to Volume Rendering</title><link href="http://localhost:4000/Volume-Rendering/" rel="alternate" type="text/html" title="A Radiance Field Approach to Volume Rendering" /><published>2022-02-21T00:00:00+05:30</published><updated>2022-02-21T00:00:00+05:30</updated><id>http://localhost:4000/Volume-Rendering</id><content type="html" xml:base="http://localhost:4000/Volume-Rendering/"><![CDATA[<p>The term rendering refers to a set of techniques used to generate 2D projections from 3D data of a scene. Most multi view image synthesis pipelines can be divided into 2 blocks. First, scene representation block which constructs a 3D scene representation from multi view images of a given scene. Second, a volume rendering block which is used to generate novel view images of the scene from the learnt 3D scene representation. Depending on the type of representation used in the first block, there exist several volume rendering approaches. In this blog, we explain a radiance field based volume rendering approach.</p>

<p><img src="/images/volume_rendering.jpg" alt="_config.yml" />
<em>Figure 1 : When queried with a viewing direction, a rendering block generates a 2D projection of the scene <a class="citation" href="#DifferentiableRendering">[1]</a>.</em></p>

<h2 id="volume-rendering">Volume Rendering</h2>

<p>A radiance field \(\mathcal{L}\left(\mathbf{x}, \boldsymbol{\theta}\right)\) is a function which maps a point in 3D space \((\mathbf{x})\) and the viewing direction \((\boldsymbol{\theta})\) to a color \(\left(\mathbf{c}\right)\). In practice, the direction \(\boldsymbol{\theta}\) is given by the unit vector \(\mathbf{d}\). So radiance field will be \(\mathcal{L}\left(\mathbf{x}, \mathbf{d}\right)\). Consider points along a ray \(\left(\mathbf{r}\right)\) given by \(\mathbf{x}\left(t\right) = \mathbf{o} + t \mathbf{d}\) where \(\mathbf{o}\) is the origin and \(t \in [t_n, t_f]\) such that \(t_n\) is near bound and \(t_f\) is far bound. The expected color of this ray will be given by</p>

\[C \left(\mathbf{r} \right) = \int_{t_n}^{t_f} \mathbf{c}\left(\mathbf{x}\left(t\right), \mathbf{d} \right) dt\]

<p>In the above equation, we have not considered whether the point is on a particle or not. Intuitively, the expected color of the ray should be influenced more by the colors on the particles. This can be incorporated by weighing color at each point with a term that gives high value if a particle is present and a low value otherwise. We will use volume density \(\sigma\left(\mathbf{x}\right)\) as the weighing term which can be interpreted as the probability of a ray terminating on a particle at \(\mathbf{x}\). The expected color will now be given by</p>

\[\begin{equation}
    C\left(\mathbf{r}\right) = \int_{t_n}^{t_f} \sigma\left(\mathbf{x}\left(t\right)\right) \mathbf{c}\left(\mathbf{x}\left(t\right), \mathbf{d}\right) dt
\end{equation}\]

<p>Once a ray has hit an opaque particle at \(t_i\), it terminates. So points beyond \(t_i\) will not contribute to the expected color of the ray. For this, we will introduce a term known as accumulated transmittance which gives the probability of a ray travelling from \(t_n\) to \(t\) without hitting any particle. It is given by</p>

\[\begin{equation}
    T\left(t\right) = \text{exp}\left(-\int_{t_n}^t \sigma\left(\mathbf{x}\left(s\right)\right) ds\right)
    \tag{1}
\end{equation}\]

<p>Incorporating all the factors, the expected color of a ray \(\mathbf{r}\) will be given by</p>

\[\begin{equation}
    C\left(\mathbf{r}\right) = \int_{t_n}^{t_f} T\left(t\right) \sigma\left(\mathbf{x}\left(t\right)\right) \mathbf{c}\left(\mathbf{x}\left(t\right), \mathbf{d}\right) dt
    \tag{2}
\end{equation}\]

<p>Since it is practically not possible to have a continuous set of points, the above integral is numerically estimated using quadrature rule <a class="citation" href="#QuadratureFormulaBlog">[2]</a> for a discrete set of points \(\{t_i\}_{i=1}^N\). We define \(\delta_i = t_{i+1} - t_i\) as the distance between adjacent samples. The estimated accumulated transmittance \(\hat{T}\) and \(\hat{C}\) are given by</p>

\[\begin{equation}
    T_i = \text{exp}\left(-\sum_{j=1}^{i-1} \sigma_j \delta_j \right)
    \tag{3}
\end{equation}\]

\[\begin{equation}
    \hat{C}\left(r\right) = \sum_{i=1}^{N} T_i \left(1 - \text{exp}\left( -\sigma_i \delta_i \right) \right) \mathbf{c}_i
    \tag{4}
\end{equation}\]

<p>The above equation can be interpreted as a linear combination of colors along the sampled points on the ray. Thus the expected color can be written as</p>

\[\begin{equation}
    \hat{C} = \sum\limits_{i=1}^{N} w_i \mathbf{c}_i
    \tag{5}
    \label{eq:weighted}
\end{equation}\]

<p>where \(w_i\) is the weight given to \(\mathbf{c}_i\) and is given by \(w_i = T_i \left(1 - \text{exp}\left( -\sigma_i \delta_i \right) \right)\). The weight is made up of two terms with the first term taking into account historical information and the second term the current information.</p>

<p>When radiance field gives output in RGB color space i.e. \(\mathbf{c} \in \mathbb{R}^3\) is an RGB vector, the output of volume rendering is the final image. But, when \(\mathbf{c} \in \mathbb{R}^d\) is a feature vector, the output of volume rendering is a feature image. Feature images contain more information that can be used for image space refinement <a class="citation" href="#GIRAFFE">[3]</a>. This allows rendering at lower resolutions and then upsampling to required resolution using convolutional layers. Rendering at lower resolutions helps in faster training and maintaining interactive frame rates.</p>

<h2 id="sampling-points-along-a-ray">Sampling points along a ray</h2>

<p>The color of each pixel in the generated image is found from the ray terminating at that pixel. Hence to accurately estimate the pixel color, we ideally would need the color information at every point along that ray from \([t_n, t_f]\). Since it is not practically possible to consider every point along the ray, we sample points along it.</p>

<p><img src="/images/termination.png" alt="_config.yml" />
<em>Figure 2 : Uniform sampling of points results in most points not contributing to the expected color of the ray. <a class="citation" href="#TermiNERF">[4]</a></em></p>

<p>The two most commonly used sampling techniques are Stratified sampling and Hierarchical volume sampling or Importance sampling <a class="citation" href="#NERF">[5]</a>. Consider a ray \(\mathbf{r}\) originating from the point \(\mathbf{o}\). The stratified sampling approach builds upon uniform sampling wherein we take \(N\) equidistant points along the ray. Here, we uniformly divide \(\mathbf{r}\) into \(N\) evenly spaced bins \(\{b_i\}_{i=1}^{N}\) and then draw one sample uniformly at random from each bin. In this approach the number of points sampled will be equal to the number of bins.</p>

\[\begin{equation}
   t_i \sim U \left[ t_n + \frac{i-1}{N} ( t_f - t_n ), t_n + \frac{i}{N} ( t_f -t_n )  \right]
\end{equation}\]

<p>One evident drawback of this approach is that it picks one sample from each bin without considering the contribution of that sample to the final image. This approach also samples from free spaces and occluded regions along the path of \(\mathbf{r}\) that do not contribute to the rendered image. To address this issue a hierarchical sampling approach is used which increases rendering efficiency by allocating samples proportionally to their expected effect on the final rendering.</p>

<p>In Hierarchical / Importance sampling approach, two sets of points are sampled. First, a set of \(N_c\) locations are sampled using stratified sampling and a “coarse” network is evaluated at these locations using equation \(\ref{eq:weighted}\). The expected coarse color \(\hat{C}_c\) of the ray can be formally written as:</p>

\[\hat{C_c} = \sum_{i=1}^{N_c} w_i \mathbf{c}_i\]

<p>The “coarse” output color \(\hat{C}_c\) is refined by re-sampling points along the ray. This “fine” re-sampling is performed by taking into account \(w_i\), which is a direct indication of the contribution of each of the previously sampled \(N_c\) points.</p>

<p>While re-sampling, the task is to have more samples around the points for which \(w_i\) is higher. This is achieved by performing inverse transform sampling.</p>

<h3 id="inverse-transform-sampling">Inverse transform sampling</h3>

<p>Given a uniform distribution \(\mathcal{U}(0,1)\) and a target distribution \(f_X(x)\), inverse transform sampling finds a transformation \(T\) such that, \(T(\mathcal{U}) \sim f_X(x)\).</p>

<p><img src="/images/ITS.jpg" alt="_config.yml" />
<em>Figure 3 : Inverse transform sampling is used to find the transformation which converts uniform distribution to an arbitrary target distribution.</em></p>

<p>If we consider \(F_X(x)\) as the CDF of the target distribution, the relation between \(T\) and \(F_X(x)\) can be found as follows:</p>

\[F_x(x) = P ( X \leq x ) = P ( T ( U ) \leq x ) = P ( U \leq T^{-1}(x) ) = T^{-1}(x)\]

<p>Hence, the required transformation is the inverse of the CDF of the target distribution.</p>

<p>This approach can be employed for re-sampling the points by transforming \(\mathcal{U} \left[ 0 , 1 \right]\) to get the target PDF \(f_X(x)\) which can be obtained by normalizing the weights \(w_i\).</p>

\[f_X(x) = \frac{w_i}{\sum\limits_{j=1}^{N_c} w_j } ~~~~ x \in b_i\]

<p>Hence, sampling \(F_X^{-1}(x)\) will generate points closer to the initial set of samples whose corresponding \(w_i\) was higher. The second set of \(N_f\) locations sampled from this distribution along with the initial \(N_c\) samples are used to evaluate the “fine” network. The final rendered color of the ray can be found by evaluating equation \(\ref{eq:weighted}\) at all \(N_c + N_f\) samples.</p>

<h2 id="summary">Summary</h2>
<p>Given a 3D scene representation, the task of a rendering block is to generate 2D projections of the scene. This block estimates the color of each pixel in the output image by sampling points along the ray which terminates at that pixel. This blog explains in detail, a radiance field based volume rendering approach and sampling strategies associated with it.</p>

<h2 id="references">References</h2>
<ol class="bibliography"><li><span id="DifferentiableRendering">M. Niemeyer, L. Mescheder, M. Oechsle, and A. Geiger, <a href="https://arxiv.org/abs/1912.07372v1">“Differentiable Volumetric Rendering: Learning Implicit 3D Representations Without 3D Supervision,”</a> in <i>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2020, pp. 3501–3512.</span></li>
<li><span id="QuadratureFormulaBlog"><a href="http://encyclopediaofmath.org/index.php?title=Gauss_quadrature_formula&amp;oldid=43647">“Gauss quadrature formula.”</a> Encyclopedia of Mathematics.</span></li>
<li><span id="GIRAFFE">M. Niemeyer and A. Geiger, <a href="https://arxiv.org/abs/2011.12100">“GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields,”</a> in <i>2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2021, pp. 11448–11459.</span></li>
<li><span id="TermiNERF">M. Piala and R. Clark, <a href="https://arxiv.org/abs/2111.03643">“TermiNeRF: Ray Termination Prediction for Efficient Neural Rendering,”</a> in <i>2021 International Conference on 3D Vision (3DV)</i>, 2021, pp. 1106–1114.</span></li>
<li><span id="NERF">B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, <a href="https://arxiv.org/abs/2003.08934">“NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis,”</a> in <i>ECCV</i>, 2020.</span></li></ol>]]></content><author><name>Sidharth Nair, Meher Abhijeet</name></author><summary type="html"><![CDATA[The term rendering refers to a set of techniques used to generate 2D projections from 3D data of a scene. Most multi view image synthesis pipelines can be divided into 2 blocks. First, scene representation block which constructs a 3D scene representation from multi view images of a given scene. Second, a volume rendering block which is used to generate novel view images of the scene from the learnt 3D scene representation. Depending on the type of representation used in the first block, there exist several volume rendering approaches. In this blog, we explain a radiance field based volume rendering approach.]]></summary></entry><entry><title type="html">3D Scene Representation</title><link href="http://localhost:4000/Scene-Representations/" rel="alternate" type="text/html" title="3D Scene Representation" /><published>2022-01-22T00:00:00+05:30</published><updated>2022-01-22T00:00:00+05:30</updated><id>http://localhost:4000/Scene-Representations</id><content type="html" xml:base="http://localhost:4000/Scene-Representations/"><![CDATA[<p>Rendering multiview images of a scene has been a long standing research area in the field of computer vision. Given multiple images of a scene with varying viewing directions, the task of multiview image synthesis is to generate photorealistic images of the scene with novel views.</p>

<p><img src="/images/Nerf_1.PNG" alt="_config.yml" />
<em>Figure 1 : Given multiple input images of a scene with varying viewing directions, a multi view rendering network <a class="citation" href="#NERF">[1]</a> renders new views of the scene.</em></p>

<p>Before rendering an image, we first try to represent the scene using an intermediate representation that provides 3D information. Based on the characteristics of these representations they are divided into Explicit, Implicit and Hybrid. In this blog we try to intuitively explain Explicit, Implicit and Hybrid representations as a build up to explaining the method proposed in Efficient Geometry-aware 3D Generative Adversarial Networks <a class="citation" href="#3DGAN">[2]</a>.</p>

<h2 id="implicit-representation">Implicit Representation</h2>
<p>The word implicit means “suggested though not directly expressed”. Following this nuance, in implicit representations the 3D information is not definitely expressed. Rather, the 3D scene information is represented using a function. So this forms the ideal recipe for supervised learning where we learn the 3D information by feeding the network with the scene information represented using the camera parameters. Although it seems to be a perfect way to represent a scene, it has its own de-merits.</p>

<p>Before exploring its efficiency, we will look at NeRF (Neural Radiance Fields) <a class="citation" href="#NERF">[1]</a> as an example for implicit representation. In this work, a deep fully connected network learns the color \(( \begin{bmatrix} R &amp;G &amp;B \end{bmatrix}^T \in \mathbb{R}^3)\) and density \((\sigma \in \mathbb{R})\) information of the scene as a function of the location of a point \(\begin{bmatrix} x &amp;y &amp;z \end{bmatrix}^T\) along a ray and the viewing direction of the scene \(\begin{bmatrix} \theta &amp;\phi \end{bmatrix}^T\). In practice, viewing direction is expressed as a 3D unit vector. The network is learnt by overfitting on a scene i.e. showing images of the same scene captured from different views. During inference, the network regresses the color and density information when queried with a novel camera location and direction.</p>

<p><img src="/images/Nerf_2.PNG" alt="_config.yml" />
<em>Figure 2 : A point on the ray along with its viewing direction as shown in (a) is passed through \(F_{\theta}\), which is trained to regress the color and density information as shown in (b). This regressed density and color which vary along the ray (c) is now passed through a volume rendering network to generate an image of the scene which satisfies the underlying viewing direction, \(\begin{bmatrix} \theta &amp;\phi \end{bmatrix}^T\). The entire network comprising of \(F_{\theta}\) and rendering network is trained using the rendering loss which is the squared \(L_2\) norm between the generated image and the the ground truth image (d).</em></p>

<p>To intuitively understand the output color and density information given by the network, consider a camera ray passing through the scene as shown in Figure 2. The final color information that we see on the image formed is all embedded in this ray. The final color contributed by this ray is the accumulation of all the color surfaces this ray passes through before reaching the camera. And on its way to the camera the ray can pass through different types of transparent, opaque and translucent surfaces. The network essentially tries to trace out the path through which the ray has travelled and this path can be expressed using the color and density information. The output color gives the RGB information of a point \(\begin{bmatrix} x &amp;y &amp;z \end{bmatrix}^T\) on the path while the output density is the probability of a surface being present at that point. Hence, by combining the color and density information we can reconstruct the scene.</p>

<p>Although using this method we get the complete information of the scene, the downside to this approach is its inference time. For querying one input we would need an forward pass through the deep MLP network. And hence explicit representations were introduced to address this issue.</p>

<h2 id="explicit-representation">Explicit Representation</h2>

<p>Unlike the implicit representation explained above, in this method we express 3D data using explicitly defined shapes like spheres, cubes, cuboids etc. In this approach, we represent the surface by learning a function \(f\) which maps input images to a 3D grid \(f : \mathbb{R}^2 \rightarrow \mathbb{R}^3\). An example of such a 3D representation is Voxel grid. Voxels are essentially 3D pixels shaped in the form of perfect cubes.</p>

<p><img src="/images/voxel_1.jpeg" alt="_config.yml" />
<em>Figure 3 : Difference between a pixel and voxel. For a side of length \(N\), a pixel representation requires \(O\left(N^2\right)\) memory whereas a voxelized representation requires \(O\left(N^3\right)\) memory.</em>
<img src="/images/voxel_2.jpg" alt="_config.yml" />
<em>Figure 4 : \(A01 – A05\) are examples of how voxel grids of different resolutions model the surface of a car <a class="citation" href="#VoxelBlog">[3]</a>.</em></p>

<p>To understand how a voxel grid represents a 3D scene, consider the car shown in figure 4. If we were to approximate this car’s surface using a 3D voxel grid, in the most basic way we fill up each voxel of the grid with a \(1\) or \(0\) to represent whether or not a part of the car surface is present at that voxel location. Hence using this binary method to populate the voxel grid, we can form a very coarse way to approximate the car’s surface. To also represent finer surface textures, we can increase the number of voxels in the voxel grid there by increasing the number of voxels corresponding to a particular point on the surface.</p>

<p><img src="/images/3D-R2N2.jpg" alt="_config.yml" />
<em>Figure 5 : An overview of a model <a class="citation" href="#3D-R2N2">[4]</a> that takes a sequence of images (or just one image) from arbitrary viewpoints of a scene and generates voxelized 3D reconstruction as output.</em></p>

<p>In theory, voxel grid is a fast technique for modelling complex surfaces. We can accurately replicate real world objects by employing a proper rendering technique like <a class="citation" href="#3D-R2N2">[4]</a> armed with a very high resolution voxel grid. This approach addresses the inference time problem associated with the implicit representation because in this case we can query the voxel grid in \(O(1)\) time. However this method is inefficient in terms of memory usage as voxel grid requires \(O(N^3)\) memory for a side of length \(N\).</p>

<h2 id="hybrid-representation">Hybrid Representation</h2>

<p>This method takes the best of the two approaches mentioned above and hence is also called explicit-implicit representation. As an example, we will consider Tri-plane hybrid representation <a class="citation" href="#3DGAN">[2]</a> which gives color and volume density as the final 3D representation. As shown in figure 6, first, a generator <a class="citation" href="#StyleGAN2">[5]</a> modulated using latent vectors and camera parameters generates explicit features. These features are then projected along three axis-aligned orthogonal feature planes. Second, the projected features are aggregated and fed through a small MLP based light weight decoder generating color and volume density. During inference, a query 3D position \((\mathbf{x} \in \mathbb{R}^3)\) is projected onto each of the three feature planes retrieving the corresponding feature vectors \((F_{xy}, F_{xz}, F_{yz})\).</p>

<p><img src="/images/tri-plane-hybrid.jpg" alt="_config.yml" />
<em>Figure 6 : Sub network from <a class="citation" href="#3DGAN">[2]</a> which generates the tri-plane 3D representation</em></p>

<p>For a side of length \(N\), the tri-plane representation generates explicit features \(\in \mathbb{R}^{N \times N \times C}\) along each plane, where \(C\) is the number of channels. This implies, tri-plane representation scales with \(O\left(N^2\right)\) while Voxel grid scales with \(O\left(N^3\right)\). As a consequence, tri-plane representation can use higher resolution features and capture greater detail at the cost of a smaller memory. This allows the model to shift bulk of the expressive power of 3D representation to the explicit features while keeping the decoder network small. Thus, the computational cost inccured during inference is much less when compared to implicit representation like NeRF <a class="citation" href="#NERF">[1]</a>.</p>

<h2 id="summary">Summary</h2>

<p>The networks shown in figure 7, summarize the three aproaches mentioned above. The implicit network is accurate but computationally heavy. On the other hand the explicit network is computationally light but memory intensive and the novel hybrid approach finds the middle ground by being light weight and computationally fast.</p>

<p><img src="/images/hybrid_1.jpg" alt="_config.yml" />
<em>Figure 7 : Comparison between (a) Implicit, (b) Explicit and (c) Hybrid representations.</em></p>

<h2 id="references">References</h2>
<ol class="bibliography"><li><span id="NERF">B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron, R. Ramamoorthi, and R. Ng, <a href="https://arxiv.org/abs/2003.08934">“NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis,”</a> in <i>ECCV</i>, 2020.</span></li>
<li><span id="3DGAN">E. R. Chan <i>et al.</i>, <a href="https://arxiv.org/abs/2112.07945">“Efficient Geometry-aware 3D Generative Adversarial Networks,”</a> in <i>arXiv</i>, 2021.</span></li>
<li><span id="VoxelBlog"><a href="http://www.bilderzucht.de/blog/3d-pixel-voxel/">“3D Pixel / Voxel.”</a> .</span></li>
<li><span id="3D-R2N2">C. B. Choy, D. Xu, J. Y. Gwak, K. Chen, and S. Savarese, <a href="https://arxiv.org/abs/1604.00449">“3D-R2N2: A Unified Approach for Single and Multi-view 3D Object Reconstruction,”</a> in <i>Proceedings of the European Conference on Computer Vision (ECCV)</i>, 2016.</span></li>
<li><span id="StyleGAN2">T. Karras, S. Laine, M. Aittala, J. Hellsten, J. Lehtinen, and T. Aila, <a href="https://arxiv.org/abs/1912.04958">“Analyzing and Improving the Image Quality of StyleGAN,”</a> in <i>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</i>, 2020, pp. 8107–8116.</span></li></ol>]]></content><author><name>Sidharth Nair, Meher Abhijeet</name></author><summary type="html"><![CDATA[Rendering multiview images of a scene has been a long standing research area in the field of computer vision. Given multiple images of a scene with varying viewing directions, the task of multiview image synthesis is to generate photorealistic images of the scene with novel views.]]></summary></entry></feed>